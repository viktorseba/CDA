{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To embed plots in the notebooks\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np # numpy library\n",
    "import scipy.linalg as lng # linear algebra from scipy library\n",
    "from scipy.spatial import distance # load distance function\n",
    "from sklearn import preprocessing as preproc # load preprocessing function\n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetPath = './DiabetesDataNormalized.txt'\n",
    "T = np.loadtxt(diabetPath, delimiter = ' ', skiprows = 1)\n",
    "y = T[:, 10]\n",
    "X = T[:,:10]\n",
    "\n",
    "# Get number of observations (n) and number of independent variables (p)\n",
    "[n, p] = np.shape(X)\n",
    "\n",
    "M = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Solve the Ordinary Least Squares (OLS) computationally (for the diabetes data set):\n",
    "\n",
    "> (a) What is the difference between using a brute force implementation(analytical) for an OLS solver and a numerically ’smarter’ implementation? Compute the ordinary least squares solution to the diabetes data set for both options and look at the relative difference. Use for example lng.lstsq to invert the matrix or to solve the linear system of equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_numerical(X, y):\n",
    "    # Call lstsq from lng to get betas\n",
    "    # beta, residues, rank, sing_val = lng.lstsq(a=X, b=y)\n",
    "    return lng.lstsq(a=X, b=y)\n",
    "\n",
    "\n",
    "def ols_analytical(X, y):\n",
    "    # Implement the analytical closed form way of calculating the betas \n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    }
   ],
   "source": [
    "# numerical solution\n",
    "beta_num, _, _, _ = ols_numerical(X, y)\n",
    "print(f'The list of betas: \\n{beta_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n"
     ]
    }
   ],
   "source": [
    "# analytical solution\n",
    "beta_ana = ols_analytical(X,y)\n",
    "print(f'The list of betas: \\n{beta_ana}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of the difference between betas: \n",
      "2.1112011768850182e-14\n"
     ]
    }
   ],
   "source": [
    "# difference in solutions\n",
    "norm = np.linalg.norm(beta_ana-beta_num)\n",
    "print(f'The norm of the difference between betas: \\n{norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the difference significant? \n",
    "\n",
    "What can we conclude relating to numerical vs analytical solutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) How could you include an intercept term in Python? This means using the model: $y = β_0 +xβ_1 +...+x_pβ_p +ε $ rather than: $ y=x_1β_1 +...+x_pβ_p +ε. $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n",
      "The list of betas: \n",
      "[-0.00618293 -0.14813008  0.32110005  0.20036692 -0.48931352  0.29447365\n",
      "  0.06241272  0.10936897  0.46404908  0.04177187]\n",
      "The norm of the difference between betas: \n",
      "4.330529936130431e-15\n"
     ]
    }
   ],
   "source": [
    "# Include offset / intercept\n",
    "M = np.hstack(((np.ones_like(X[:,0]))[:, np.newaxis], X))\n",
    "\n",
    "# numerical solution\n",
    "beta_num_bias, _, _, _ = ols_numerical(M, y)\n",
    "print(f'The list of betas: \\n{beta_num}')\n",
    "\n",
    "# analytical solution\n",
    "beta_ana_bias = ols_analytical(M,y)\n",
    "print(f'The list of betas: \\n{beta_ana}')\n",
    "\n",
    "# difference in solutions\n",
    "norm = np.linalg.norm(beta_ana_bias-beta_num_bias)\n",
    "print(f'The norm of the difference between betas: \\n{norm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the value of the intercept coefficient?\n",
    "\n",
    "Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\"> Value of intercept: about 0\n",
    "\n",
    "<span style=\"color:yellow\"> Reason: OLS is an unbiased estimator. Therefore the intercept (bias) is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) Calculate the mean squared error $MSE = 1/n \\sum^n_{i=1} (y_i−x_iβ)^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the estimated y values and use these to calculate the MSE.\n",
    "def compute_mse(X,beta,y):\n",
    "    y_hat = X @ beta\n",
    "    res = y - y_hat\n",
    "    mse = np.mean((res)**2)\n",
    "    return mse, res, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the analytical solution: 0.48116051086159695\n"
     ]
    }
   ],
   "source": [
    "mse_ana, res_ana, yhat_ana = compute_mse(X,beta_ana,y)\n",
    "\n",
    "print(f'mse from the analytical solution: {mse_ana}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens to the MSE if we change some of the betas?\n",
    "\n",
    "Is that what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse from the changed betas: 0.5676790520132494\n"
     ]
    }
   ],
   "source": [
    "beta_new = beta_ana\n",
    "beta_new[5] = 0\n",
    "\n",
    "mse_new, res_new, yhat_new = compute_mse(X,beta_new,y)\n",
    "\n",
    "print(f'mse from the changed betas: {mse_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\"> If we change value sof beta, we get a worse MSE. This makes sense as OLS optimizes a squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (d) Calculate the residual sum of squares $RSS = ∥{\\bf y} − Xβ∥_2^2$ and the total sum of squares $T SS = ∥{\\bf y} − y∥_2^2$, where $y$ is the estimated mean of ${\\bf y}$. Report on the $R^2$ measure, that is, the proportion of variance in the sample set explained by the\n",
    "  model: $R^2 = 1 − \\frac{RSS}{TSS}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51774842222035"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RSS = np.linalg.norm(y - M@beta_num_bias)**2\n",
    "TSS = np.linalg.norm(y - np.mean(M@beta_num_bias))**2\n",
    "\n",
    "R2 = 1 - RSS/TSS\n",
    "R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much variance in <strong>y</strong> can we explain using this simple model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\"> With the simple model, we can explain about 52% of the varience in y </span>"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": ".venv_ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
