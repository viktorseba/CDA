{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests \n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('sand.mat')\n",
    "X = mat['X']\n",
    "y = mat['Y'].ravel()\n",
    "\n",
    "[n, p] = X.shape\n",
    "\n",
    "def centerData(data):\n",
    "    \n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    \n",
    "    return data, mu\n",
    "\n",
    "def normalize(X):\n",
    "    d = np.linalg.norm(X, ord=2, axis=0)\n",
    "    return X / d, d\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Perform univariate feature selection for the sand data using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Bonferroni correction to control the family-wise error rate(FWER). Use FWER = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the pvalue for each feature one at the time because OLS breaks down with this many features\n",
    "# Use the stats models linear regression, since p value already is included\n",
    "# Otherwise check https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "# Which explains how to expand the class in sklearn to calculate it\n",
    "X_norm,_ = centerData(X)\n",
    "X_norm,_ = normalize(X_norm)\n",
    "y_norm,_ = centerData(y)\n",
    "\n",
    "pvals = np.zeros(p)\n",
    "for j in range(p):\n",
    "    slope, intcpt, r_val, pvals[j], stderr= linregress(X_norm[:, j], y_norm)\n",
    "\n",
    "\n",
    "# Sort p-values in acending order\n",
    "# pvals.sort()\n",
    "\n",
    "# include all features with p values lower  than p / features\n",
    "features_to_include = np.arange(p)[np.sort(pvals) < 0.05/p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_to_include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Benjamini-Hochbergâ€™s algorithm for FDR. Use an acceptable fraction of mistakes,\n",
    "q = 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use multipletests  to get the FDR corrected p values\n",
    "reject, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh', is_sorted=False, returnsorted=False)\n",
    "# Sort p-values in acending order\n",
    "\n",
    "# include all features with p values lower  than q\n",
    "features_to_include = np.arange(p)[np.sort(pvals_corrected) < 0.15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_to_include)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the solutions in terms of number of selected features and selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is clear that FDR \"allows\" for more features to be kept in the model, and through this the chance of having false discoveries are higher, this is done to make sure that all significant features are kept in the model, whereas bonferroni might remove some significant features because of the more stringent cutoff.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
