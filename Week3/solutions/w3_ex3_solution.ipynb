{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests \n",
    "\n",
    "# seaborn can be used to \"prettify\" default matplotlib plots by importing and setting as default\n",
    "import seaborn as sns\n",
    "sns.set() # Set searborn as default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('sand.mat')\n",
    "X = mat['X']\n",
    "y = mat['Y'].ravel()\n",
    "\n",
    "[n, p] = X.shape\n",
    "\n",
    "def centerData(data):\n",
    "    \n",
    "    mu = np.mean(data,axis=0)\n",
    "    data = data - mu\n",
    "    \n",
    "    return data, mu\n",
    "\n",
    "def normalize(X):\n",
    "    '''\n",
    "    Function for normalizing the columns (variables) of a data matrix to unit length.\n",
    "    Returns the normalized data and the euclidian lenghts of the variables \n",
    "    \n",
    "    Input  (X) --------> The data matrix to be normalized \n",
    "    Output (X_pre)-----> The normalized data matrix \n",
    "    Output (d) --------> Array with the euclidian lenghts of the variables \n",
    "    '''\n",
    "    d = np.linalg.norm(X,axis=0,ord=2)  # d is the the L2 norms of the variables\n",
    "    d[d==0]=1                           # Avoid dividing by zero if column L2 norm is 0 \n",
    "    X_pre = X / d                       # Normalize the data with the euclidian lengths\n",
    "    return X_pre,d                      # Return normalized data and the euclidian lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Perform univariate feature selection for the sand data using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Bonferroni correction to control the family-wise error rate(FWER). Use FWER = 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after correcting with Bonferroni correction: 72.\n"
     ]
    }
   ],
   "source": [
    "PValues = np.zeros(p)\n",
    "Xsub = np.zeros(p)\n",
    "for j in range(p):\n",
    "    Xsub = X[:,j]\n",
    "    # Use the stats models linear regression, since p value already is included\n",
    "    # Otherwise check https://stackoverflow.com/questions/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "    # Which explains how to expand the class in sklearn to calculate it\n",
    "    slope, intercept, r_value, PValues[j], std_err = linregress(Xsub, y)\n",
    "\n",
    "# Sort p-values in acending order\n",
    "idx1 = np.argsort(PValues)\n",
    "p = PValues[idx1]\n",
    "\n",
    "remaining_features_bonf = len(np.where(p < (0.05 / 2016))[0]) # Amount af features included\n",
    "print(f'Remaining features after correcting with Bonferroni correction: {remaining_features_bonf}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Benjamini-Hochbergâ€™s algorithm for FDR. Use an acceptable fraction of mistakes,\n",
    "q = 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining features after applying DFR: 721.\n"
     ]
    }
   ],
   "source": [
    "FDR = multipletests(PValues, alpha = 0.05, method = \"fdr_bh\")[1] # Computing Benjamini Hochberg's FDR\n",
    "\n",
    "idx2 = np.argsort(FDR)\n",
    "fdr = FDR[idx2]\n",
    "\n",
    "remaining_features_fdr = len(np.where(fdr < 0.15)[0]) # How many values are below 0.15?\n",
    "print(f'Remaining features after applying DFR: {remaining_features_fdr}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the solutions in terms of number of selected features and selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It is clear that FDR \"allows\" for more features to be kept in the model, and through this the chance of having false discoveries are higher, this is done to make sure that all significant features are kept in the model, whereas bonferroni might remove some significant features because of the more stringent cutoff.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
